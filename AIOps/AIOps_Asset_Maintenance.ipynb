{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Predictive Assets Maintenance with BigDL Time Series Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we demonstrate how to use `TCNForecaster` and `ThresholdDetector` to make predictions, anomaly detections and therefore assets maintenance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration, we use the publicly available elevator predictive maintenance dataset. You can find the dataset introduction <a href=\"https://www.kaggle.com/datasets/shivamb/elevator-predictive-maintenance-dataset\" target=\"_blank\">here</a>. The target is to predict an absolute value of vibration. Then maintenance teams can be alerted to inspect and address potential issues proactively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before runnning the notebook, you need to download the <a href=\"https://www.kaggle.com/datasets/shivamb/elevator-predictive-maintenance-dataset\" target=\"_blank\">dataset</a> from kaggle and decompress it to get a csv file called `predictive-maintenance-dataset.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section provides a helper function to plot the ground truth, prediction and anomaly value. You can refer to it later when in use."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def plot_anomalies_value(y_true, y_pred, pattern_ano_index, trend_ano_index, threshold):\n",
    "    \"\"\"\n",
    "    Plot the ground truth, prediction and anomaly value.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\"y_true\": y_true.squeeze(), \"y_pred\": y_pred.squeeze()})\n",
    "    df['p_ano_index'] = 0\n",
    "    df.loc[df.index[pattern_ano_index], 'ano_index'] = 1\n",
    "    df['t_ano_index'] = 0\n",
    "    df.loc[df.index[trend_ano_index], 'ano_index'] = 1\n",
    "    df['threshold'] = threshold\n",
    "\n",
    "    fig, axs = plt.subplots(figsize=(16,6))\n",
    "    axs.plot(df.index, df.y_true, color='blue', label='Ground Truth')\n",
    "    axs.plot(df.index, df.y_pred, color='orange', label='Prediction')\n",
    "    axs.plot(df.index, df.threshold, color='black', label='Threshold')\n",
    "    axs.scatter(df.index[pattern_ano_index].tolist(), df.y_true[pattern_ano_index], color='red', label='checking points for pattern anomaly')\n",
    "    axs.scatter(df.index[trend_ano_index].tolist(), df.y_true[trend_ano_index], color='green', label='checking points for trend anomaly')\n",
    "    axs.set_title('Checking Points For Maintenance')\n",
    "    \n",
    "    plt.xlabel('time_step')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Download raw dataset and load into dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the <a href=\"https://www.kaggle.com/datasets/shivamb/elevator-predictive-maintenance-dataset\" target=\"_blank\">dataset</a> from kaggle and decompress it to get a csv file called `predictive-maintenance-dataset.csv`. Use pandas to load `predictive-maintenance-dataset.csv` into a dataframe as shown below."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "df = pd.read_csv(\"/dataset/predictive-maintenance-dataset.csv\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Below are some example records of the data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T15:27:42.023958Z",
     "start_time": "2024-12-03T15:27:41.882889Z"
    }
   },
   "source": [
    "df.head()"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdf\u001B[49m\u001B[38;5;241m.\u001B[39mhead()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'df' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "df.plot(y=\"vibration\", x=\"ID\", figsize=(16,6), title=\"vibration\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we need to do data cleaning and preprocessing on the raw data. Note that this part and the following part could vary for different datasets. \n",
    "\n",
    "For the elevator data, the pre-processing converts the time step to timestamp starting from 2023-01-01 16:30:00."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "df[\"time_step\"] = pd.date_range(start='2023-01-01 16:30:00', end='2023-01-01 23:30:00', periods=len(df))\n",
    "df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering & Data Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We scale and roll the data to generate the sample in numpy ndarray for `TCNForecaster` to use.\n",
    "\n",
    "We use <a href=\"https://bigdl.readthedocs.io/en/latest/doc/PythonAPI/Chronos/tsdataset.html\" target=\"_blank\">TSDataset</a> to complete the whole processing."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from bigdl.chronos.data import TSDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "lookback = 120\n",
    "horizon = 1\n",
    "\n",
    "tsdata_train, tsdata_val, tsdata_test = TSDataset.from_pandas(df, dt_col=\"time_step\", target_col=\"vibration\",\n",
    "                                                              extra_feature_col=[\"revolutions\",\"humidity\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\"],\n",
    "                                                              with_split=True, test_ratio=0.1)\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "for tsdata in [tsdata_train, tsdata_test]:\n",
    "    tsdata.scale(standard_scaler, fit=(tsdata is tsdata_train))\\\n",
    "          .roll(lookback=lookback, horizon=horizon)\n",
    "\n",
    "x_train, y_train = tsdata_train.to_numpy()\n",
    "x_test, y_test = tsdata_test.to_numpy()\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Time series forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we initialize a TCNForecaster based on time step and feature number. More information about TCNForecaster can be found <a href=\"https://bigdl.readthedocs.io/en/latest/doc/PythonAPI/Chronos/forecasters.html#tcnforecaster\" target=\"_blank\">here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some industrial scenarios, such as this one, the adverse effect caused by a predicted value being less than the real value is far greater than that caused by a predicted value being greater than the real value. Therefore, in this case, we use a built-in loss function `AsymWeightLoss` to penalize underestimation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from bigdl.chronos.forecaster import TCNForecaster\n",
    "from bigdl.chronos.pytorch.loss import AsymWeightLoss"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "forecaster = TCNForecaster(past_seq_len=lookback,\n",
    "                           future_seq_len=horizon,\n",
    "                           input_feature_num=8,\n",
    "                           output_feature_num=1,\n",
    "                           normalization=False,\n",
    "                           kernel_size=5,\n",
    "                           num_channels=[16]*8,\n",
    "                           loss=AsymWeightLoss(underestimation_penalty=10))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we train the model and wait until it's finished."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "print('Start training ...')\n",
    "forecaster.num_processes = 1\n",
    "forecaster.fit(data=tsdata_train, epochs=5)\n",
    "print('Training completed')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Then we can use the fitted forecaster for prediction and inverse the scaling of the prediction results."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "y_pred_train = forecaster.predict(x_train)\n",
    "y_pred_test = forecaster.predict(x_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "y_pred_train_unscale = tsdata_train.unscale_numpy(y_pred_train)\n",
    "y_pred_test_unscale = tsdata_test.unscale_numpy(y_pred_test)\n",
    "y_train_unscale = tsdata_train.unscale_numpy(y_train)\n",
    "y_test_unscale = tsdata_test.unscale_numpy(y_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from bigdl.chronos.metric.forecast_metrics import Evaluator\n",
    "metric = Evaluator.evaluate('mse', y_test_unscale, y_pred_test_unscale)\n",
    "print(f\"MSE is {'%.2f' % metric[0]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking points detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we initiate a ThresholdDetector to detect checking points, i.e. anomaly needed to pay attention. More information about ThresholdDetector can be found <a href=\"https://bigdl.readthedocs.io/en/latest/doc/PythonAPI/Chronos/anomaly_detectors.html#thresholddetector\" target=\"_blank\">here</a>. Based on the trainning dataset, we can train it to obtain some information about a threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, in this case, we can set the absolute threshold of vibration and detect potential elevator failure."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import math\n",
    "from bigdl.chronos.detector.anomaly import ThresholdDetector\n",
    "thd = ThresholdDetector()\n",
    "vibration_th = 85\n",
    "thd.set_params(trend_threshold=(0, vibration_th)) # if vibration>85, we think there may exist potential elevator failure\n",
    "thd.fit(y_train_unscale, y_pred_train_unscale)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We detect two types of anomalies, i.e. pattern anomaly and trend anomaly. By comparing real data and predicted data, we find those pattern anomalies. Meanwhile, we also support forecasting anomaly, which is a detected trend anomaly of predicted data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case1**: If we only have predicted data and want to forecast an anomaly"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "test_anomaly_indexes = thd.anomaly_indexes(y_pred=y_pred_test_unscale)\n",
    "print(\"The index of anomalies in test dataset only according to predict data is:\")\n",
    "for key, value in test_anomaly_indexes.items():\n",
    "    print(f'{key}: {value}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `plot_anomalies_value` to intuitively see the detection results."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plot_anomalies_value(y_test_unscale, y_pred_test_unscale, test_anomaly_indexes['pattern anomaly index'], test_anomaly_indexes['trend anomaly index'], vibration_th)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case2**: If we have true data, predicted data and want to detect an anomaly"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "test_anomaly_indexes = thd.anomaly_indexes(y=y_test_unscale, y_pred=y_pred_test_unscale)\n",
    "print(\"The index of anomalies in test dataset according to true data and predicted data is:\")\n",
    "for key, value in test_anomaly_indexes.items():\n",
    "    print(f'{key}: {value}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `plot_anomalies_value` to intuitively see the detection results."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plot_anomalies_value(y_test_unscale, y_pred_test_unscale, test_anomaly_indexes['pattern anomaly index'], test_anomaly_indexes['trend anomaly index'], vibration_th)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can focus on these checking points to avoid asset loss in time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "ba8bfd3bf16964ea3765573d7d6a9e7a0b4091a3f490749e514ae33be4d38eb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
